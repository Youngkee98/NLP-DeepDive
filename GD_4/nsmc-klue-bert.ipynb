{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e08b488",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° íŒ¨í‚¤ì§€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20f25fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177d4356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b41227923f24cd8ab3583c23e0157b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ë¡œë”© ì¤‘ ë¡œê·¸ ì¶œë ¥ ìµœì†Œí™” (ë¡œë”©ë°” í¬í•¨)\n",
    "import datasets\n",
    "datasets.logging.set_verbosity_error()  # ëª¨ë“  ë¡œê¹… ì¶œë ¥ (ë¡œë”©ë°” í¬í•¨)ì„ ë”\n",
    "\n",
    "# ì‹¤ì œ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "nsmc_dataset = load_dataset('e9t/nsmc')\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì „ì²´ êµ¬ì¡° ì¶œë ¥ (train/validation/testë¡œ ë‚˜ë‰œ dict í˜•íƒœ)\n",
    "print(nsmc_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d0c0f7",
   "metadata": {},
   "source": [
    "### âœ… ì²´í¬í¬ì¸íŠ¸ : ë°ì´í„° êµ¬ì„±ìš”ì†Œ ë¶„ì„\n",
    "\n",
    "---\n",
    "\n",
    "ë°í„°ì…‹ì€ DatasetDict í˜•íƒœë¡œ train, test ê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "`train` : 150000ê°œì˜ ë°ì´í„°, `id`,`document`,`label` ì´ ì¡´ì¬í•œë‹¤.\n",
    "`test` : 50000ê°œì˜ ë°ì´í„°, `id`,`document`,`label` ì´ ì¡´ì¬í•œë‹¤.\n",
    "\n",
    "**Validation Setì„ ë‚˜ëˆ ì¤˜ì•¼ê² ë‹¤. í•™ìŠµìš©ìœ¼ë¡œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc72305f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 135000\n",
      "}), 'validation': Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 15000\n",
      "}), 'test': Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 50000\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "# train ë°ì´í„°ë¥¼ 90% í•™ìŠµìš© / 10% ê²€ì¦ìš©ìœ¼ë¡œ ë‚˜ëˆ”\n",
    "# ì¼ë°˜ì ìœ¼ë¡œ 10% ì •ë„ ì‚¬ìš©í•œë‹¤ê³  í•œë‹¤.\n",
    "nsmc_split = nsmc_dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# ìƒˆë¡­ê²Œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥\n",
    "nsmc_dataset = {\n",
    "    'train': nsmc_split['train'],\n",
    "    'validation': nsmc_split['test'],\n",
    "    'test': nsmc_dataset['test']\n",
    "}\n",
    "\n",
    "print(nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14df3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative (0): 67703\n",
      "Positive (1): 67297\n"
     ]
    }
   ],
   "source": [
    "# í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸í•˜ê¸°\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# train splitì—ì„œ label ì¶”ì¶œ\n",
    "train_labels = nsmc_dataset['train']['label']\n",
    "\n",
    "# ê° í´ë˜ìŠ¤ ê°œìˆ˜ ì„¸ê¸°\n",
    "label_counts = Counter(train_labels)\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"Negative (0): {label_counts[0]}\")\n",
    "print(f\"Positive (1): {label_counts[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1540a2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfV0lEQVR4nO3de5BdZZnv8e9PQsIlSKJgyyRIZyQFFVCYdAs4qNXNNUSGcErHgWNJGBkz54jCGeUMZCgHRBl0dEStUWbQIAEZGowwpBDFGGkpzxAgHQKBIKQJJKQrkUvCpSXmwjznj/U2bHbvnexe2df071O1q/d61mU/a/Xl6Xetd61XEYGZmVkeb2t0AmZm1rpcRMzMLDcXETMzy81FxMzMcnMRMTOz3FxEzMwsNxcRMzPLzUXErEokPSPpOUn7FsT+RlJvej9L0nJJr0h6QdKvJU1J8y6XFJIuLNrmhSl+eUHs7ZK+LWmtpEFJT6XpA+qzp2ZvchExq649gAuLg5IOBW4AvgjsD0wBvge8XrDYk8A5RavOTvGh7YwFFgNHADOAtwMfBF4EjqnWTphVykXErLq+AVwkaUJR/Gjg6YhYHJlXI+KnEbG2YJkHgX0kHQGQvu6V4kPOAd4D/I+IWBkR/x0Rz0XEVyLirlrtlFk5LiJm1bUU6AUuKoovAw6XdLWkbknjy6x/I2+2Rman6UInAb+IiMEq5Wu2S1xEzKrvH4HPSzpwKBARq4EuYBJwK/CCpOtLFJMfA2dL2hM4K00XeiewvlaJm42Ui4hZlUXEo8CdwCVF8SUR8YmIOBD4MPAR4NKiZdYC/cA/Aasi4tmizb8IHFSr3M1GykXErDYuAz5D1vIYJiIeBG4Djiwxe+gC/A0l5v0KOLWwB5hZI7mImNVARPQDtwAXAEj6kKTPSHpXmj4cOANYUmL1W4BTyE57FbsReBb4qaTDJb1N0jsl/YOkmbXYF7MdcRExq50rgKEWw0tkRWOFpEHgF8DtwD8XrxQRmyPiVxGxucS8LWQX138HLAJeAR4ADgDur8E+mO2QPCiVmZnl5ZaImZnl5iJiZma5uYiYmVluLiJmZpbbmEYnUG8HHHBAtLe351r3D3/4A/vu2/zd81shz1bIEZxnNbVCjuA8y+nr63sh3Sj7VhExql4dHR2R1z333JN73XpqhTxbIccI51lNrZBjhPMsB1gaJf6m+nSWmZnl5iJiZma5uYiYmVluLiJmZpabi4iZmeXmImJmZrnVrIhIuk7Sc5IeLYh9Q9LvJD0i6fbCcaglzZXUL+kJSacWxGekWL+kSwriUyTdn+K3SBpbq30xM7PSatkSuR6YURRbBBwZEe8HngTmAkiaRjYU6BFpne9L2kPSHsD3gNOAaWTDhk5L2/o6cHVEHApsAs6r4b6YmVkJNSsiEXEvsLEo9suI2J4mlwCT0/tZQE9EbImIp8mGBz0mvfojYnVEbAV6gFmSBJwALEjrzwfOrNW+mJlZaY187MmnyUZwg2wI0cIR3tbx5rCizxbFjwXeCbxUUJAKlx9G0hxgDkBbWxu9vb25Eh4cHMy9biUeWbGCbVu3DovvOXYs73/f+yreTq3zrIZWyBGcZzW1Qo7gPEeqIUVE0qXAduCmenxeRFwLXAvQ2dkZXV1dubbT29tL3nUr0d3dzVXLnh8Wv2j6gcQIBg+rdZ7V0Ao5gvOsplbIEZznSNW9iEg6FzgdODHe/Ms4ABxcsNjkFKNM/EVggqQxqTVSuLyZmdVJXbv4SpoB/D1wRkS8VjBrIXCWpHGSpgBTycaNfhCYmnpijSW7+L4wFZ97gI+n9WcDd9RrP8zMLFPLLr43A/cBh0laJ+k84F+B/YBFkpZL+jeAiHgMuBVYCfwCOD8iXk+tjM8BdwOPA7emZQEuBr4gqZ/sGsm8Wu2LmZmVVrPTWRFxdolw2T/0EXElcGWJ+F3AXSXiq8l6b5mZWYP4jnUzM8vNRcTMzHJzETEzs9xcRMzMLDcXETMzy81FxMzMcnMRMTOz3FxEzMwsNxcRMzPLzUXEzMxycxExM7PcXETMzCw3FxEzM8vNRcTMzHJzETEzs9xcRMzMLDcXETMzy81FpAWMGTsOScNekw9pb3RqZjbK1Wx4XKue7Vu3cNWy54fF504/sAHZmJm9yS0RMzPLzUXEzMxycxExM7PcXETMzCw3FxEzM8vNRcTMzHJzETEzs9xqVkQkXSfpOUmPFsTeIWmRpFXp68QUl6TvSuqX9Iik6QXrzE7Lr5I0uyDeIWlFWue7klSrfTEzs9Jq2RK5HphRFLsEWBwRU4HFaRrgNGBqes0BroGs6ACXAccCxwCXDRWetMxnCtYr/iwzM6uxmhWRiLgX2FgUngXMT+/nA2cWxG+IzBJggqSDgFOBRRGxMSI2AYuAGWne2yNiSUQEcEPBtszMrE6U/Q2u0calduDOiDgyTb8UERPSewGbImKCpDuBr0XEb9O8xcDFQBewV0R8NcW/BGwGetPyJ6X4h4GLI+L0MnnMIWvh0NbW1tHT05NrfwYHBxk/fnyudSvR19fHpGlHDYsPrHy4bLyjo2NYvNZ5VkMr5AjOs5paIUdwnuV0d3f3RURncbxhz86KiJBUuwr21s+6FrgWoLOzM7q6unJtp7e3l7zrVqK7u7v0M7Jmnlw2XuqfgFrnWQ2tkCM4z2pqhRzBeY5UvXtn/T6diiJ9fS7FB4CDC5abnGI7ik8uETczszqqdxFZCAz1sJoN3FEQPyf10joOeDki1gN3A6dImpguqJ8C3J3mvSLpuHRa7JyCbZmZWZ3UsovvzcB9wGGS1kk6D/gacLKkVcBJaRrgLmA10A/8APgsQERsBL4CPJheV6QYaZkfpnWeAn5eq31pVuXGGenr6/NYI2ZWFzW7JhIRZ5eZdWKJZQM4v8x2rgOuKxFfChy5Kzm2unLjjEzasJyBtWsakJGZjTa+Y93MzHJzETEzs9xcRMzMLDcXETMzy81FxMzMcnMRMTOz3FxEzMwsNxcRMzPLzUXEzMxycxExM7PcXETMzCw3F5HdVLmHM/rBjGZWTQ0blMpqq9zDGedOP7AB2ZjZ7sotETMzy81FxMzMcnMRMTOz3FxEzMwsNxcRMzPLzUXEzMxycxExM7PcXETMzCw3FxEzM8vNRcTMzHJzETEzs9xcRMzMLLeGFBFJfyfpMUmPSrpZ0l6Spki6X1K/pFskjU3LjkvT/Wl+e8F25qb4E5JObcS+mJmNZnUvIpImARcAnRFxJLAHcBbwdeDqiDgU2AScl1Y5D9iU4len5ZA0La13BDAD+L6kPeq5L63Ij4g3s2pq1KPgxwB7S9oG7AOsB04A/meaPx+4HLgGmJXeAywA/lWSUrwnIrYAT0vqB44B7qvTPrQkPyLezKpJEVH/D5UuBK4ENgO/BC4ElqTWBpIOBn4eEUdKehSYERHr0ryngGPJCsuSiPhxis9L6ywo8XlzgDkAbW1tHT09PbnyHhwcZPz48bnWrURfXx+Tph01LD6w8uERxcdt28zqVU+OeFsdHR05Mx+5Wh/LanGe1dMKOYLzLKe7u7svIjqL43VviUiaSNaKmAK8BPyE7HRUzUTEtcC1AJ2dndHV1ZVrO729veRdtxLd3d2lWwkzTx5RvH3Dci666KIRb6ue/1DU+lhWi/OsnlbIEZznSDXiwvpJwNMR8XxEbANuA44HJkgaKmqTgYH0fgA4GCDN3x94sTBeYh0zM6uDRhSRtcBxkvZJ1zZOBFYC9wAfT8vMBu5I7xemadL8X0f2L/NC4KzUe2sKMBV4oE77YGZmNOB0VkTcL2kBsAzYDjxEdqrpZ0CPpK+m2Ly0yjzgxnThfCNZjywi4jFJt5IVoO3A+RHxel13xsxslGtI76yIuAy4rCi8mqx3VfGyfwT+ssx2riS7QG+7aKjrb7FJ7zmEdWueqX9CZtYSGtXF15qMu/6aWR5+7ImZmeXmImJmZrm5iJiZWW47vSYiaQJwDtBeuHxEXFCzrMzMrCVUcmH9LmAJsAL479qmY2ZmraSSIrJXRHyh5pmYmVnLqeSayI2SPiPpIEnvGHrVPDNrCn50vJntSCUtka3AN4BLgaEn9AXwp7VKypqH7x8xsx2ppIh8ETg0Il6odTKjxeRD2hlYu6bRaZiZ7bJKikg/8FqtExlNBtau8X/3ZrZbqKSI/AFYLukeYMtQ0F18zcyskiLyn+llZmb2FjstIhExvx6JmJlZ66nkjvWnebNX1hsiwr2zzMxGuUpOZxUOzL4X2dgevk/EzMx2frNhRLxY8BqIiG8DH619amZm1uwqOZ01vWDybWQtEw9mZWZmFRWDfyl4vx14BvhETbIxM7OWUknvrO56JGKtxWOymxlUdjprHPAxho8nckXt0rJm52dqmRlUdjrrDuBloI+CO9bNzMwqKSKTI2JGzTMxM7OWU8l4Iv8l6X01z8TMzFpOJS2RDwHnpjvXtwACIiLeX9PMzMys6VXSEjkNmAqcAvwFcHr6mpukCZIWSPqdpMclfTCNmLhI0qr0dWJaVpK+K6lf0iOF961Imp2WXyVp9q7kZGZmI1fJHetrSr128XO/A/wiIg4HjgIeBy4BFkfEVGBxmoY3i9hUYA5wDUAaovcy4FjgGOCyocJjZmb1UUlLpKok7Q98BJgHEBFbI+IlYBYw9MTg+cCZ6f0s4IbILAEmSDoIOBVYFBEbI2ITsAhwBwAzszqqexEBpgDPAz+S9JCkH0raF2iLiPVpmQ1AW3o/CXi2YP11KVYubmZmdaKIYU95f+sC0hRgfUT8MU3vTfYH/5lcHyh1AkuA4yPifknfAV4BPh8REwqW2xQREyXdCXwtIn6b4ouBi4EuYK+I+GqKfwnYHBHfLPGZc8hOhdHW1tbR09OTJ3UGBwcZP358rnUL9fX1MWnaUcPiAysfrkp83LbNrF71ZE0/Y0fxjo6OYfFi1TqWteY8q6cVcgTnWU53d3dfRHQWxyspIkuBP4+IrWl6LPD/IuIDeRKR9G5gSUS0p+kPk13/OBToioj16XRVb0QcJunf0/ub0/JPkBWQrrT836b4W5Yrp7OzM5YuXZondXp7e+nq6sq1biFJZe/2rka8fcNyzp55ck0/Y0fxnf1MQfWOZa05z+pphRzBeZYjqWQRqeR01pihAgLZNQxgbN5EImID8Kykw1LoRGAlsBAY6mE1m+xOeVL8nNRL6zjg5XTa627gFEkT0wX1U1LMzMzqpJL7RJ6XdEZELASQNAt4YRc/9/PATalVsxr4a7KCdquk84A1vPmk4LuAmUA/8FpalojYKOkrwINpuSsiYuMu5mVmZiNQSRH5X2R/8L+Xpp8FPrUrHxoRy3nriIlDTiyxbADnl9nOdcB1u5KLmZnlV8mj4J8CjpM0Pk0P1jwrMzNrCTu9JiJpf0nfAnqBXkn/ku71MDOzUa6SC+vXAa+SXaP4BFl33B/VMinb/Uw+pB1Jb7z6+vqQxORD2hudmpntgkquibw3Ij5WMP1lSctrlI/tpgbWrnlLl+BJG5Zz1bLnPYiVWYurpCWyWdKHhiYkHQ9srl1KZmbWKippifxvYH66DiJgI3BuLZMyM7PWUEnvrOXAUZLenqZfqXVSZmbWGsoWEUlfKBMHICK+VaOczMysReyoJbJf+noY8AGyx49ANiDVA7VMyszMWkPZIhIRXwaQdC8wPSJeTdOXAz+rS3a22xszdtwbrdtCk95zCOvWPFP/hMxsRCq5sN4GbC2Y3sqbY32Y7ZLtW7eUfRqwmTW/SorIDcADkm5P02cC19cqITMzax2VjLF+JdmTczel119HxFW1TsxGt6HTXMUv3+Fu1lwqaYkQEcuAZTXOxewNPs1l1hoaMcb6qFH8vKihl5nZ7qKilojlU/y8qCH+b9rMdhduiZiZWW4uImZmlpuLiJmZ5eYiYmZmubmImJlZbi4iZmaWm4uImZnl5iJiZma5uYiYmVluLiJmZpZbw4qIpD0kPSTpzjQ9RdL9kvol3SJpbIqPS9P9aX57wTbmpvgTkk5t0K6YmY1ajWyJXAg8XjD9deDqiDiU7JHz56X4ecCmFL86LYekacBZwBHADOD7kvaoU+5mZkaDioikycBHgR+maQEnAAvSIvPJBr8CmJWmSfNPTMvPAnoiYktEPA30A8fUZQfMzAwARUT9P1RaAFwF7AdcBJwLLEmtDSQdDPw8Io6U9CgwIyLWpXlPAccCl6d1fpzi89I6C4o+DklzgDkAbW1tHT09PbnyHhwcZPz48RUv39fXx6RpRw2LD6x8uKbxcds2s3rVkw357IGVD9PR0TEsXnwsxm3bzJY9967a9mtlpN/zRmmFPFshR3Ce5XR3d/dFRGdxvO5FRNLpwMyI+KykLupQRAp1dnbG0qVLc+Xe29tLV1dXxctLKvso+FrG2zcs5+yZJzfks+dOP5BSP1PFx6J9w3KeeffRVdt+rYz0e94orZBnK+QIzrMcSSWLSCNOZx0PnCHpGaCH7DTWd4AJkobGN5kMDKT3A8DBAGn+/sCLhfES69SVB58ys9Gq7oNSRcRcYC7AUEskIj4p6SfAx8kKy2zgjrTKwjR9X5r/64gISQuB/5D0LeBPgKnAA3XclTd48CkzG62aaWTDi4EeSV8FHgLmpfg84EZJ/cBGsh5ZRMRjkm4FVgLbgfMj4vX6p22Fxowd51aY2SjS0CISEb1Ab3q/mhK9qyLij8Bflln/SuDK2mVoI7V96xa3ysxGEd+xbmZmubmImJlZbi4iZmaWm4uImZnl5iJiZma5uYiYmVluLiJmZpabi4iZmeXmImJmZrm5iJiZWW4uImZmlpuLiJmZ5eYiYmZmubmImJlZbi4iZmaWm4uImZnl5iJiLWVo5MTi1+RD2hudmtmo1EzD45rtlEdONGsubomYmVluLiJmZpabi4iZmeXmImJmZrm5iJiZWW4uImZmlpuLiJmZ5Vb3IiLpYEn3SFop6TFJF6b4OyQtkrQqfZ2Y4pL0XUn9kh6RNL1gW7PT8qskza73vpiZjXaNaIlsB74YEdOA44DzJU0DLgEWR8RUYHGaBjgNmJpec4BrICs6wGXAscAxwGVDhcfMzOqj7kUkItZHxLL0/lXgcWASMAuYnxabD5yZ3s8CbojMEmCCpIOAU4FFEbExIjYBi4AZ9dsTMzNTRDTuw6V24F7gSGBtRExIcQGbImKCpDuBr0XEb9O8xcDFQBewV0R8NcW/BGyOiG+W+Jw5ZK0Y2traOnp6enLlOzg4yPjx44fF+/r6mDTtqGHxgZUPNyQ+bttmVq96sqlyKo6P27aZLXvuXdXtd3R0DIvvqnLf82bTCnm2Qo7gPMvp7u7ui4jO4njDioik8cBvgCsj4jZJLw0VkTR/U0RMrEYRKdTZ2RlLly7NlXNvby9dXV2l9qXs85waEW/fsJyzZ57cVDkVx9s3LOeZdx9d1e3X4me53Pe82bRCnq2QIzjPciSVLCIN6Z0laU/gp8BNEXFbCv8+naYifX0uxQeAgwtWn5xi5eJmZlYnjeidJWAe8HhEfKtg1kJgqIfVbOCOgvg5qZfWccDLEbEeuBs4RdLEdEH9lBQzM7M6aURL5HjgU8AJkpan10zga8DJklYBJ6VpgLuA1UA/8APgswARsRH4CvBgel2RYjYKeZwR211NPqS95M/2IytWNDo1oAHjiaRrGyoz+8QSywdwfpltXQdcV73srFV5nBHbXQ2sXVPyZ3vbr+eXWLr+fMe6mZnl5iJiu7Vyp7l8qsusOjw8ru3Wyp3mAp/qMqsGt0RG4JEVK0r+R2tmNlq5JTIC27Zu9cVbM7MCbonYqOVuwWa7zi0RG7XcLdhs17klYlaksIXS19fnForZDrglYlaksIUyacPyN967hWI2nFsiZhXyNRSz4dwSMatQuWsoXzpucsmu3pPecwjr1jxTh8zMGsdFxGwXubjYaOYiYlYj7v1lo4GviZiZNYFyj3xvdm6JmDWJyYe0M7B2zbD42L33Yevm14bFfVps91Luke/N3nJ1ETFrEjv6I9KKf1xsdPDpLLMWVe6myHH77OuuyFY3bomYtagd3RQ5kt5i4FNmlp+LiFmdDbUg6m1nY6v4lFl9lLv21apcRMzqrJW6/pYreOVaLjtq0fx4/vW1SLHltOoF9HJcRMysrB0VPHcC2LHdrcVRjouImdXcmLHj6Ovro7u7+y3xPC2aRl2jKVcUyuW0u7U4ynERMbOa2751C5OmHTXsj2q1WjQjvcdmR/F/+soVw4rdkJF2WBgNXETMrOXlucemXLxUsRuaV0orXeOqBRcRM2sZjerZZuW5iJhZyxjt//U3o5a/Y13SDElPSOqXdEmj8zEzG01auohI2gP4HnAaMA04W9K0xmZlZjZ6tHQRAY4B+iNidURsBXqAWQ3Oycys5ko9H60Rz05TRNRkw/Ug6ePAjIj4mzT9KeDYiPhc0XJzgDlp8jDgiZwfeQDwQs5166kV8myFHMF5VlMr5AjOs5xDImLYxadRcWE9Iq4Frt3V7UhaGhGdVUipplohz1bIEZxnNbVCjuA8R6rVT2cNAAcXTE9OMTMzq4NWLyIPAlMlTZE0FjgLWNjgnMzMRo2WPp0VEdslfQ64G9gDuC4iHqvhR+7yKbE6aYU8WyFHcJ7V1Ao5gvMckZa+sG5mZo3V6qezzMysgVxEzMwsNxeRCjTro1UkHSzpHkkrJT0m6cIUf4ekRZJWpa8TmyDXPSQ9JOnOND1F0v3pmN6SOkY0OscJkhZI+p2kxyV9sEmP5d+l7/ejkm6WtFczHE9J10l6TtKjBbGSx0+Z76Z8H5E0vcF5fiN93x+RdLukCQXz5qY8n5B0aqNyLJj3RUkh6YA03bBjCS4iO9Xkj1bZDnwxIqYBxwHnp9wuARZHxFRgcZputAuBxwumvw5cHRGHApuA8xqS1Vt9B/hFRBwOHEWWb1MdS0mTgAuAzog4kqxDyVk0x/G8HphRFCt3/E4DpqbXHOCaOuUIpfNcBBwZEe8HngTmAqTfp7OAI9I6309/ExqRI5IOBk4B1haEG3ksXUQq0LSPVomI9RGxLL1/leyP3iSy/OanxeYDZzYkwUTSZOCjwA/TtIATgAVpkWbIcX/gI8A8gIjYGhEv0WTHMhkD7C1pDLAPsJ4mOJ4RcS+wsShc7vjNAm6IzBJggqSDGpVnRPwyIranySVk95wN5dkTEVsi4mmgn+xvQt1zTK4G/h4o7BHVsGMJLiKVmAQ8WzC9LsWaiqR24M+A+4G2iFifZm0A2hqVV/Jtsh/8/07T7wReKvilbYZjOgV4HvhROu32Q0n70mTHMiIGgG+S/Se6HngZ6KP5jueQcsevmX+vPg38PL1vmjwlzQIGIuLholkNzdFFZDcgaTzwU+D/RMQrhfMi68PdsH7ckk4HnouIvkblUKExwHTgmoj4M+APFJ26avSxBEjXFGaRFb0/AfalxGmPZtQMx29nJF1Kdpr4pkbnUkjSPsA/AP/Y6FyKuYjsXFM/WkXSnmQF5KaIuC2Ffz/UnE1fn2tUfsDxwBmSniE7FXgC2bWHCel0DDTHMV0HrIuI+9P0ArKi0kzHEuAk4OmIeD4itgG3kR3jZjueQ8odv6b7vZJ0LnA68Ml48wa6ZsnzvWT/ODycfpcmA8skvZsG5+gisnNN+2iVdG1hHvB4RHyrYNZCYHZ6Pxu4o965DYmIuRExOSLayY7dryPik8A9wMfTYg3NESAiNgDPSjoshU4EVtJExzJZCxwnaZ/0/R/Ks6mOZ4Fyx28hcE7qWXQc8HLBaa+6kzSD7JTrGRHxWsGshcBZksZJmkJ28fqBeucXESsi4l0R0Z5+l9YB09PPbWOPZUT4tZMXMJOsx8ZTwKWNzqcgrw+RnR54BFieXjPJrjksBlYBvwLe0ehcU75dwJ3p/Z+S/TL2Az8BxjVBfkcDS9Px/E9gYjMeS+DLwO+AR4EbgXHNcDyBm8mu02wj+yN3XrnjB4is1+NTwAqy3maNzLOf7LrC0O/RvxUsf2nK8wngtEblWDT/GeCARh/LiPBjT8zMLD+fzjIzs9xcRMzMLDcXETMzy81FxMzMcnMRMTOz3FxEzEZI0uWSLqrCdgarkU/RNo+WNLNguiq5mpXjImK2ezma7F4hs7pwETGrgKRLJT0p6bfAYQXxoyUtKRiHYmi8jEMl/UrSw5KWSXrvTrb/fyU9mLbz5RRrVzauyQ+UjR/yS0l7p3kfSMsuT2NhPJqeqHAF8Fcp/ldp89Mk9UpaLemCWhwfG71cRMx2QlIH2SNbjib7L/8DBbNvAC6ObByKFcBlKX4T8L2IOAr4c7K7j8tt/xSyx2kckz6jQ9JH0uypaTtHAC8BH0vxHwF/GxFHA69D9vh6sgf03RIRR0fELWnZw4FT0/YvS89bM6sKFxGznfswcHtEvBbZU5IXwhtjkEyIiN+k5eYDH5G0HzApIm4HiIg/xlufx1TslPR6CFhG9kd/apr3dEQsT+/7gPY06t5+EXFfiv/HTvL/WWTjYbxA9gDERg8NYLuRMTtfxMxqTMBVEfHvbwlmY8RsKQi9DuydY/vF2/DvvVWNWyJmO3cvcKakvVMr4y8AIuJlYJOkD6flPgX8JrJRJtdJOhMgPQF2nx1s/27g02lcGCRNkvSucgtHNuLiq5KOTaGzCma/Cuw30h00y8tFxGwnIhuC+BbgYbIR7x4smD0b+IakR8iuZ1yR4p8CLkjx/wLevYPt/5LslNR9klaQjWWys0JwHvADScvJBqZ6OcXvIbuQXnhh3axm/BRfsxYkaXxEDKb3lwAHRcSFDU7LRiGfGzVrTR+VNJfsd3gNcG5j07HRyi0RMzPLzddEzMwsNxcRMzPLzUXEzMxycxExM7PcXETMzCy3/w9yfByYkalZTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. ë¬¸ì¥ ê¸¸ì´ ì¸¡ì •\n",
    "train_docs = nsmc_dataset['train']['document']\n",
    "lengths = [len(doc) for doc in train_docs]  # ê° ë¬¸ì¥ì˜ ë¬¸ì ìˆ˜\n",
    "\n",
    "# 2. íˆìŠ¤í† ê·¸ë¨ ê·¸ë¦¬ê¸°\n",
    "plt.hist(lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"NSMC\")\n",
    "plt.xlabel(\"doc length\")\n",
    "plt.ylabel(\"doc num\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa04c667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ í‰ê·  ë¬¸ì¥ ê¸¸ì´: 35.19\n",
      "ğŸ“Œ í‘œì¤€í¸ì°¨: 29.57\n",
      "ğŸ“Œ 90%ì˜ ë¬¸ì¥ì„ í¬í•¨í•˜ê¸° ìœ„í•œ ìµœì†Œ ê¸¸ì´: 75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. ê¸¸ì´ ë¦¬ìŠ¤íŠ¸ ìƒì„± \n",
    "lengths = [len(doc) for doc in nsmc_dataset['train']['document']]\n",
    "\n",
    "# 2. í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°\n",
    "mean_length = np.mean(lengths)\n",
    "std_length = np.std(lengths)\n",
    "\n",
    "print(f\"ğŸ“Œ í‰ê·  ë¬¸ì¥ ê¸¸ì´: {mean_length:.2f}\")\n",
    "print(f\"ğŸ“Œ í‘œì¤€í¸ì°¨: {std_length:.2f}\")\n",
    "\n",
    "# 3. ëˆ„ì  ë¶„í¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 90% ì»¤ë²„í•˜ëŠ” ê¸¸ì´ êµ¬í•˜ê¸°\n",
    "percentile_90 = np.percentile(lengths, 90)  # ì „ì²´ ë¬¸ì¥ ì¤‘ 90%ê°€ ì´ ê¸¸ì´ ì´ë‚´\n",
    "\n",
    "print(f\"ğŸ“Œ 90%ì˜ ë¬¸ì¥ì„ í¬í•¨í•˜ê¸° ìœ„í•œ ìµœì†Œ ê¸¸ì´: {percentile_90:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ad9e4",
   "metadata": {},
   "source": [
    "### âœ… ì²´í¬í¬ì¸íŠ¸ : NSMC ë°ì´í„°ê°€ ì–´ë–¤ ë°ì´í„°ì¸ì§€ íŒŒì•…\n",
    "\n",
    "---\n",
    "#### ë°ì´í„° êµ¬ì¡°\n",
    "NSMCëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„° íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆë‹¤.\n",
    "\n",
    "`ID` : ê³ ìœ  ë²ˆí˜¸\n",
    "`Document` : ë¦¬ë·° í…ìŠ¤íŠ¸ ë°ì´í„°\n",
    "`Label` : ê°ì„±ë¶„ë¥˜ 1ì€ ê¸ì • 0ì€ ë¶€ì •\n",
    "\n",
    "#### í´ë˜ìŠ¤ ë¶ˆê· í˜• ì—¬ë¶€\n",
    "í´ë˜ìŠ¤ ë¶ˆê· í˜•ì€ ê±°ì˜ ì—†ë‹¤ê³  ë³´ë©´ ëœë‹¤.\n",
    "\n",
    "#### ë¬¸ì¥ê¸¸ì´ ë¶„í¬ \n",
    "\n",
    "ğŸ“Œ í‰ê·  ë¬¸ì¥ ê¸¸ì´: 35.20\n",
    "\n",
    "ğŸ“Œ í‘œì¤€í¸ì°¨: 29.58\n",
    "\n",
    "ğŸ“Œ 90%ì˜ ë¬¸ì¥ì„ í¬í•¨í•˜ê¸° ìœ„í•œ ìµœì†Œ ê¸¸ì´: 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb2992",
   "metadata": {},
   "source": [
    "# 2ï¸âƒ£ Tokenizer & Model\n",
    "\n",
    "---\n",
    "\n",
    "ë§Œì•½ì— í—ˆê¹…í˜ì´ìŠ¤ì— ë°ì´í„°ì…‹ì´ ì—†ë‹¤ë©´ Tensorflow_datasetì—ì„œ ë¶ˆëŸ¬ì™€ì„œ Huggingface ì— ë§ê²Œ ë°”ê¾¸ëŠ” ê³¼ì •ì„ í•˜ê² ì§€ë§Œ ì¶©ë¶„íˆ ìˆìœ¼ë¯€ë¡œ í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ë¡œ ê·¸ë˜ë„ ì§„í–‰í•œë‹¤.\n",
    "\n",
    "í˜„ì¬ ë…¸ë“œì—ì„œëŠ” `Klue/Bert Model` ë° í† í¬ë‚˜ì´ì €ë¥¼ `AutoTokenizer`ì— ë§ê²Œ ì§„í–‰í• ê±°ì„."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca6f6f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./nsmc-klue-bert-finetuned/added_tokens.json. We won't load it.\n",
      "loading file ./nsmc-klue-bert-finetuned/vocab.txt\n",
      "loading file ./nsmc-klue-bert-finetuned/tokenizer.json\n",
      "loading file None\n",
      "loading file ./nsmc-klue-bert-finetuned/special_tokens_map.json\n",
      "loading file ./nsmc-klue-bert-finetuned/tokenizer_config.json\n",
      "loading configuration file ./nsmc-klue-bert-finetuned/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./nsmc-klue-bert-finetuned/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./nsmc-klue-bert-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Hugging Faceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ğŸ”¹ 1. Tokenizer ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('./nsmc-klue-bert-finetuned')\n",
    "\n",
    "# ğŸ”¹ 2. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ë¶„ë¥˜ìš©)\n",
    "\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    './nsmc-klue-bert-finetuned',  # ì‚¬ì „ í•™ìŠµëœ klue/bert-base ëª¨ë¸ ì‚¬ìš©\n",
    "    num_labels=2                # ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜: 2ê°œ (0 ë˜ëŠ” 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5667e2",
   "metadata": {},
   "source": [
    "### âœ… ì²´í¬í¬ì¸íŠ¸ : `AutoTokenizer`ì™€ `AutoModel` ì˜ ê¸°ëŠ¥\n",
    "---\n",
    "\n",
    "`AutoTokenizer`, `AutoModel` ê¸°ëŠ¥ì„ í•œë²ˆ ì‚¬ìš©í•´ ë³¸ê±°ë‹¤\n",
    "\n",
    "```python\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "```\n",
    "\n",
    " ì´ ëª…ë ¹ì€ 'distilbert-base-uncased' ëª¨ë¸ì— ìµœì í™”ëœ í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤ (ì˜ˆ: DistilBertTokenizerFast)ë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œí•´ì„œ `huggingface_tokenizer`ì— í• ë‹¹í•©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=2                \n",
    ")\n",
    "```\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” `AutoModelForSequenceClassification` í•´ë‹¹ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ìœ„ì— ë¶„ë¥˜ìš© í—¤ë“œê°€ ë¶™ì€ êµ¬ì¡°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.\n",
    "\n",
    "ì´ ë¶€ë¶„ì€ `AutoModelForTokenClassification`, `AutoModelForQuestionAnswering` ë“± ë‹¤ì–‘í•œ taskì— ë”°ë¼ ëª¨ë¸ì´ ì¡´ì¬í•œë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dd4f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    # data['document']ëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸\n",
    "    # â†’ batched=Trueì¼ ê²½ìš° í•œ ë²ˆì— ì—¬ëŸ¬ ë¬¸ì¥ì´ ë“¤ì–´ì˜´\n",
    "\n",
    "    # 1. í…ìŠ¤íŠ¸ ì •ì œ (í•„ìš” ì‹œ)\n",
    "    texts = data['document']\n",
    "    # texts = [clean_text(t) for t in texts]\n",
    "\n",
    "    # 2. í† í¬ë‚˜ì´ì§•\n",
    "    tokenized = huggingface_tokenizer(\n",
    "        texts,                         # ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì „ë‹¬\n",
    "        truncation=True,\n",
    "        max_length=75,                # ì´ì „ ë¶„ì„ ê¸°ë°˜ ê¸¸ì´ ì„¤ì •\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    # 3. ë ˆì´ë¸” ì¶”ê°€ (ê·¸ëŒ€ë¡œ ë§¤ì¹­ë¨)\n",
    "    tokenized['label'] = data['label']\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08627553",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsmc_dataset = DatasetDict({\n",
    "    'train': nsmc_split['train'],\n",
    "    'validation': nsmc_split['test'],\n",
    "    'test': nsmc_dataset['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c724885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e257b15c3c9e4fb99f45ccd7cc9f8c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67745631953d43f296c8621fb2123960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff8ff30d7a14c74924714e8ee5db096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset = nsmc_dataset.map(transform, batched=True)\n",
    "\n",
    "hf_train_dataset = hf_dataset['train']\n",
    "hf_val_dataset = hf_dataset['validation']\n",
    "hf_test_dataset = hf_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fe80d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "print(set(hf_dataset['train']['label']))  # â†’ {0, 1}ì´ë©´ OK, {-1}ì´ë©´ ë¬¸ì œ\n",
    "print(set(hf_dataset['test']['label']))  # â†’ {0, 1}ì´ë©´ OK, {-1}ì´ë©´ ë¬¸ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581c6ac",
   "metadata": {},
   "source": [
    "### âœ… ì²´í¬í¬ì¸íŠ¸: map() í•¨ìˆ˜ì˜ ì—­í• \n",
    "---\n",
    "\n",
    "ğŸ”¹ **map() í•¨ìˆ˜ëŠ” ë¬´ì—‡ì„ í•˜ë‚˜ìš”**\n",
    "\n",
    "Hugging Face datasets.Dataset ê°ì²´ì— ìˆëŠ” ëª¨ë“  ìƒ˜í”Œì— íŠ¹ì • í•¨ìˆ˜ë¥¼ ìë™ ë°˜ë³µ ì ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë°˜ë³µë¬¸ì„ ì§ì ‘ ì“°ì§€ ì•Šì•„ë„, ë‚´ë¶€ì ìœ¼ë¡œ ìµœì í™”ëœ ë°©ì‹ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì¤ë‹ˆë‹¤.\n",
    "\n",
    "ğŸ”¹ **ì™œ batched=Trueë¥¼ ì¨ì•¼ í•˜ë‚˜ìš”?**\n",
    "\n",
    "í† í¬ë‚˜ì´ì €(huggingface_tokenizer)ëŠ” ë¬¸ì¥ ì—¬ëŸ¬ ê°œë¥¼ í•œêº¼ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆì„ ë•Œ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "batched=Trueë¥¼ ì“°ë©´ transform() í•¨ìˆ˜ì— ì—¬ëŸ¬ ìƒ˜í”Œì´ list í˜•íƒœë¡œ ì „ë‹¬ë˜ë¯€ë¡œ,\n",
    "\n",
    "â†’ data['sentence1']ëŠ” ë‹¨ì¼ ë¬¸ì¥ì´ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ë¬¸ì¥ì˜ ë¦¬ìŠ¤íŠ¸ê°€ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf36291",
   "metadata": {},
   "source": [
    "# 4ï¸âƒ£ Train & Evaluation & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b19d240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME') + '/aiffel/transformers/nsmc-klue-bert' \n",
    "\n",
    "# TrainingArguments: Trainerì— ë„˜ê²¨ì¤„ í•™ìŠµ ì„¤ì •ê°’ ì •ì˜\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                          # í•™ìŠµ ê²°ê³¼ê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬\n",
    "    evaluation_strategy=\"epoch\",        # ë§¤ epochì´ ëë‚  ë•Œë§ˆë‹¤ í‰ê°€ ì‹¤í–‰\n",
    "    learning_rate=2e-5,                 # í•™ìŠµë¥  (learning rate)\n",
    "    per_device_train_batch_size=64,      # ê° ë””ë°”ì´ìŠ¤ë‹¹ í•™ìŠµ ë°°ì¹˜ í¬ê¸°\n",
    "    per_device_eval_batch_size=64,       # í‰ê°€ ì‹œ ë°°ì¹˜ í¬ê¸°\n",
    "    num_train_epochs=3,                 # í•™ìŠµí•  ì „ì²´ ì—í­ ìˆ˜\n",
    "    weight_decay=0.01                   # ê°€ì¤‘ì¹˜ ê°ì†Œ (L2 ì •ê·œí™”ì— ì‚¬ìš©)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ae8a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)  # logits â†’ class index\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),           # ì „ì²´ ì •í™•ë„\n",
    "        'precision': precision_score(labels, preds),         # ì •ë°€ë„ (ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ì–‘ì„± ë¹„ìœ¨)\n",
    "        'recall': recall_score(labels, preds),               # ì¬í˜„ìœ¨ (ì‹¤ì œ ì–‘ì„± ì¤‘ì—ì„œ ë§ì¶˜ ë¹„ìœ¨)\n",
    "        'f1': f1_score(labels, preds)                        # ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê· \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d279c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer ê°ì²´ ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=huggingface_model,             # KLUE-BERT\n",
    "    args=training_arguments,             # í•™ìŠµ ì„¤ì • ì¸ìë“¤\n",
    "    train_dataset=hf_train_dataset,      # í•™ìŠµ ë°ì´í„°ì…‹ (í† í¬ë‚˜ì´ì¦ˆ ì™„ë£Œ)\n",
    "    eval_dataset=hf_val_dataset,         # ê²€ì¦ ë°ì´í„°ì…‹\n",
    "    compute_metrics=compute_metrics      # í‰ê°€ í•¨ìˆ˜ (accuracy, F1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44f0f165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 135000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6330\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6330' max='6330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6330/6330 1:56:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.248600</td>\n",
       "      <td>0.246010</td>\n",
       "      <td>0.899533</td>\n",
       "      <td>0.881637</td>\n",
       "      <td>0.923904</td>\n",
       "      <td>0.902276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>0.239139</td>\n",
       "      <td>0.905267</td>\n",
       "      <td>0.890451</td>\n",
       "      <td>0.925100</td>\n",
       "      <td>0.907445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.269185</td>\n",
       "      <td>0.908467</td>\n",
       "      <td>0.906403</td>\n",
       "      <td>0.911819</td>\n",
       "      <td>0.909103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6330, training_loss=0.2004366641172677, metrics={'train_runtime': 6965.4757, 'train_samples_per_second': 58.144, 'train_steps_per_second': 0.909, 'total_flos': 1.56093716925e+16, 'train_loss': 0.2004366641172677, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # í•™ìŠµ ì‹œì‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982aa4e4",
   "metadata": {},
   "source": [
    "### âœ… ì²´í¬í¬ì¸íŠ¸: í•™ìŠµ ê²°ê³¼\n",
    "---\n",
    "\n",
    "ê²°ê³¼ ì ìœ¼ë¡œ ê³¼ì í•©ë˜ê¸°ì „ì— ì™„ë£Œê°€ ë˜ì—ˆì§€ë§Œ\n",
    "\n",
    "`Accuracy` : 0.90846 \n",
    "\n",
    "`F1`       : 0.909103\n",
    "\n",
    "ìš”êµ¬ì¡°ê±´ì¸ ì •í™•ë„ 90% ì´ìƒì€ ì¶©ì¡±í–ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "ì´í›„ì—ëŠ” í˜„ì¬ Finetunning ëœ ëª¨ë¸ê³¼ ê°€ì¤‘ì¹˜ë“¤ì„ ì €ì¥í•´ ì´ë‹¤ìŒ test ì…‹ì— ëŒ€í•´ ì§„í–‰í•´ë³´ì•„ Acc, F1 ì„ ì¶œë ¥í•´ë³¼ ê²ƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eda06cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./nsmc-klue-bert-finetuned\n",
      "Configuration saved in ./nsmc-klue-bert-finetuned/config.json\n",
      "Model weights saved in ./nsmc-klue-bert-finetuned/pytorch_model.bin\n",
      "tokenizer config file saved in ./nsmc-klue-bert-finetuned/tokenizer_config.json\n",
      "Special tokens file saved in ./nsmc-klue-bert-finetuned/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./nsmc-klue-bert-finetuned/tokenizer_config.json',\n",
       " './nsmc-klue-bert-finetuned/special_tokens_map.json',\n",
       " './nsmc-klue-bert-finetuned/vocab.txt',\n",
       " './nsmc-klue-bert-finetuned/added_tokens.json',\n",
       " './nsmc-klue-bert-finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model('./nsmc-klue-bert-finetuned')\n",
    "huggingface_tokenizer.save_pretrained('./nsmc-klue-bert-finetuned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c17b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./nsmc-klue-bert-finetuned')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./nsmc-klue-bert-finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2510610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d3f7d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 05:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2749262750148773, 'eval_accuracy': 0.90454, 'eval_f1': 0.9059303494353456, 'eval_runtime': 322.0676, 'eval_samples_per_second': 155.247, 'eval_steps_per_second': 19.406}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "test_trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€\n",
    "test_result = test_trainer.evaluate(hf_test_dataset)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486566b",
   "metadata": {},
   "source": [
    "### âœ… ì²´í¬í¬ì¸íŠ¸: Eval ê²°ê³¼\n",
    "---\n",
    "\n",
    "ê²°ê³¼ ì ìœ¼ë¡œ ê³¼ì í•©ë˜ê¸°ì „ì— ì™„ë£Œê°€ ë˜ì—ˆì§€ë§Œ\n",
    "\n",
    "`Accuracy` : 0.90454\n",
    "\n",
    "`F1`       : 0.90593\n",
    "\n",
    "ìš”êµ¬ì¡°ê±´ì¸ ì •í™•ë„ 90% ì´ìƒì€ ì¶©ì¡±í–ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a95d9",
   "metadata": {},
   "source": [
    "# 5ï¸âƒ£ Bucketing & Dynamic Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb5f34",
   "metadata": {},
   "source": [
    "#### âœ… Dynamic Padding ì´ë€?\n",
    "\n",
    "ë™ì  íŒ¨ë”©(Dynamic Padding)ì€ ê° ë¯¸ë‹ˆë°°ì¹˜(batch)ì—ì„œ ê°€ì¥ ê¸´ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ paddingì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì „ì²´ max_lengthë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ì§€ ì•Šê³ , ë§¤ ë°°ì¹˜ë§ˆë‹¤ í•„ìš”í•œ ë§Œí¼ë§Œ paddingí•©ë‹ˆë‹¤.\n",
    "\n",
    "`batch = [\"ë‚˜ëŠ” ê°„ë‹¤\", \"ë‚˜ëŠ” ì˜¤ëŠ˜ ë„ì„œê´€ì— ê°‘ë‹ˆë‹¤\", \"ê°‘ë‹ˆë‹¤\"]`\n",
    "\n",
    "- ê°€ì¥ ê¸´ ë¬¸ì¥ ê¸¸ì´ê°€ 8 í† í°ì´ë©´ â†’ batch ë‚´ ëª¨ë“  ë¬¸ì¥ì„ 8ì— ë§ì¶° padding\n",
    "\n",
    "- ë‹¤ìŒ ë°°ì¹˜ê°€ ë” ì§§ì€ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ë©´ â†’ ê·¸ ë°°ì¹˜ì—ì„œëŠ” 5ë§Œí¼ë§Œ padding\n",
    "\n",
    "#### âœ… Bucketingì´ë€?\n",
    "\n",
    "ë²„í‚·íŒ…(Bucketing)ì€ ë¬¸ì¥ ê¸¸ì´ê°€ ë¹„ìŠ·í•œ ìƒ˜í”Œë¼ë¦¬ ë¬¶ì–´ì„œ batchë¥¼ êµ¬ì„±í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.\n",
    "Dynamic Paddingì˜ íš¨ìœ¨ì„ ë” ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ ë³´ì¡° ì „ëµì…ë‹ˆë‹¤.\n",
    "\n",
    "`ë°ì´í„°ì…‹ ì „ì²´: [5í† í°, 7í† í°, 100í† í°, 105í† í°, 110í† í°, 10í† í°...]`\n",
    "\n",
    "`Bucket 1: [5, 7, 10]`\n",
    "\n",
    "`Bucket 2: [100, 105, 110]`\n",
    "\n",
    "â†’ ë¹„ìŠ·í•œ ê¸¸ì´ë¼ë¦¬ batchë¥¼ êµ¬ì„±í•˜ë©´ paddingì´ ê±°ì˜ í•„ìš” ì—†ìŒ â†’ í•™ìŠµ ì†ë„ í–¥ìƒ ë° ë©”ëª¨ë¦¬ ë‚­ë¹„ ê°ì†Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c067990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. í† í¬ë‚˜ì´ì € ê¸°ë°˜ìœ¼ë¡œ dynamic padding ìˆ˜í–‰í•˜ëŠ” Data Collator ìƒì„±\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# â†’ ê° ë°°ì¹˜ë§ˆë‹¤ ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ì— ë§ì¶° paddingì„ ë™ì ìœ¼ë¡œ ìˆ˜í–‰\n",
    "# â†’ Padding íš¨ìœ¨ì„ ë†’ì—¬ GPU ë©”ëª¨ë¦¬ ë‚­ë¹„ë¥¼ ì¤„ì´ê³  í•™ìŠµ ì†ë„ í–¥ìƒ ê¸°ëŒ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b068e9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "# 3. TrainingArguments ì •ì˜ (group_by_length í¬í•¨)\n",
    "output_dir = os.getenv('HOME') + '/aiffel/transformers/nsmc-klue-bert' \n",
    "\n",
    "training_arguments_bucketing = TrainingArguments(\n",
    "    output_dir=output_dir,              # ëª¨ë¸ê³¼ ë¡œê·¸ íŒŒì¼ ì €ì¥ ìœ„ì¹˜\n",
    "    evaluation_strategy=\"epoch\",       # ë§¤ epoch ëë‚  ë•Œë§ˆë‹¤ validation í‰ê°€ ìˆ˜í–‰\n",
    "    learning_rate=2e-5,                # í•™ìŠµë¥ \n",
    "    per_device_train_batch_size=64,    # í•™ìŠµ ì‹œ ë°°ì¹˜ í¬ê¸°\n",
    "    per_device_eval_batch_size=64,     # í‰ê°€ ì‹œ ë°°ì¹˜ í¬ê¸°\n",
    "    num_train_epochs=3,                # í•™ìŠµ epoch ìˆ˜\n",
    "    weight_decay=0.01,                 # L2 ì •ê·œí™”ì— ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜ ê°ì†Œ ê°’\n",
    "    group_by_length=True               # ë¬¸ì¥ ê¸¸ì´ì— ë”°ë¼ ìœ ì‚¬í•œ ìƒ˜í”Œë¼ë¦¬ ë²„í‚·ì„ ë§Œë“¤ì–´ í•™ìŠµ\n",
    "    # â†’ ì´ë¡œ ì¸í•´ ë™ì  íŒ¨ë”© íš¨ê³¼ê°€ ê·¹ëŒ€í™”ë¨\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d3c52b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Trainer ê°ì²´ ìƒì„±\n",
    "trainer_bucketing = Trainer(\n",
    "    model=model,                             # fine-tuned ëœ KLUE-BERT ëª¨ë¸\n",
    "    args=training_arguments_bucketing,       # ìœ„ì—ì„œ ì •ì˜í•œ í•™ìŠµ ì„¤ì •\n",
    "    train_dataset=hf_train_dataset,          # í•™ìŠµ ë°ì´í„°ì…‹\n",
    "    eval_dataset=hf_val_dataset,             # ê²€ì¦ ë°ì´í„°ì…‹\n",
    "    tokenizer=tokenizer,                     # tokenizer: ë¡œê·¸ ê¸°ë¡, data_collator ë“±ì— ì‚¬ìš©ë¨\n",
    "    data_collator=data_collator,             # ë™ì  padding ì ìš©ì„ ìœ„í•œ collator\n",
    "    compute_metrics=compute_metrics          # ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ í‰ê°€ í•¨ìˆ˜\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9a5cf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 135000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6330\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6330' max='6330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6330/6330 1:58:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.332428</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.903744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.410031</td>\n",
       "      <td>0.904733</td>\n",
       "      <td>0.906005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.467877</td>\n",
       "      <td>0.905800</td>\n",
       "      <td>0.906788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /aiffel/aiffel/transformers/nsmc-klue-bert/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6330, training_loss=0.06771285311682342, metrics={'train_runtime': 7083.0432, 'train_samples_per_second': 57.179, 'train_steps_per_second': 0.894, 'total_flos': 1.56093716925e+16, 'train_loss': 0.06771285311682342, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. í•™ìŠµ ì‹œì‘ (Bucketing + Dynamic Padding ì ìš©)\n",
    "trainer_bucketing.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dc766ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 05:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Test Evaluation Result:\n",
      "{'eval_loss': 0.47763878107070923, 'eval_accuracy': 0.9001, 'eval_f1': 0.9015356107946146, 'eval_runtime': 324.192, 'eval_samples_per_second': 154.23, 'eval_steps_per_second': 19.279}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# test set í‰ê°€ìš© Trainer ìƒì„±\n",
    "test_trainer = Trainer(\n",
    "    model=trainer_bucketing.model,  # í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©\n",
    "    tokenizer=tokenizer,            # ì´ì „ì— ì €ì¥í•´ë‘” tokenizer\n",
    "    compute_metrics=compute_metrics # í‰ê°€ ì§€í‘œ í•¨ìˆ˜\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€\n",
    "test_result = test_trainer.evaluate(hf_test_dataset)\n",
    "print(\"ğŸ“Œ Test Evaluation Result:\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ae0e2",
   "metadata": {},
   "source": [
    "# âœ…ì²´í¬í¬ì¸íŠ¸ : Bucket + Dynamic Padding\n",
    "\n",
    "---\n",
    "\n",
    "**Bucket + Dynamic Padding**\n",
    "\n",
    "`eval_accuracy: 0.9001\n",
    "eval_f1      : 0.9015\n",
    "eval_runtime : 324.192`\n",
    "\n",
    "**ê¸°ì¡´**\n",
    "\n",
    "`'eval_accuracy': 0.90454\n",
    "'eval_f1': 0.9059\n",
    "'eval_runtime': 322.0676`\n",
    "\n",
    "í°ì°¨ì´ê°€ ì—†ìŒ.\n",
    "\n",
    "ê·¸ëŸ°ë° íŒ€ì›ë“¤ê³¼ ì–˜ê¸°ë¥¼ í•´ë³´ì•˜ì„ ë•Œ ë‹¬ë¼ì„œ Config í™•ì¸ë§Œ í•´ë³´ì\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18611c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_name_or_path': 'klue/bert-base',\n",
      " 'add_cross_attention': False,\n",
      " 'architectures': ['BertForMaskedLM'],\n",
      " 'attention_probs_dropout_prob': 0.1,\n",
      " 'bad_words_ids': None,\n",
      " 'bos_token_id': None,\n",
      " 'chunk_size_feed_forward': 0,\n",
      " 'classifier_dropout': None,\n",
      " 'decoder_start_token_id': None,\n",
      " 'diversity_penalty': 0.0,\n",
      " 'do_sample': False,\n",
      " 'early_stopping': False,\n",
      " 'encoder_no_repeat_ngram_size': 0,\n",
      " 'eos_token_id': None,\n",
      " 'finetuning_task': None,\n",
      " 'forced_bos_token_id': None,\n",
      " 'forced_eos_token_id': None,\n",
      " 'hidden_act': 'gelu',\n",
      " 'hidden_dropout_prob': 0.1,\n",
      " 'hidden_size': 768,\n",
      " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
      " 'initializer_range': 0.02,\n",
      " 'intermediate_size': 3072,\n",
      " 'is_decoder': False,\n",
      " 'is_encoder_decoder': False,\n",
      " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
      " 'layer_norm_eps': 1e-12,\n",
      " 'length_penalty': 1.0,\n",
      " 'max_length': 20,\n",
      " 'max_position_embeddings': 512,\n",
      " 'min_length': 0,\n",
      " 'model_type': 'bert',\n",
      " 'no_repeat_ngram_size': 0,\n",
      " 'num_attention_heads': 12,\n",
      " 'num_beam_groups': 1,\n",
      " 'num_beams': 1,\n",
      " 'num_hidden_layers': 12,\n",
      " 'num_return_sequences': 1,\n",
      " 'output_attentions': False,\n",
      " 'output_hidden_states': False,\n",
      " 'output_scores': False,\n",
      " 'pad_token_id': 0,\n",
      " 'position_embedding_type': 'absolute',\n",
      " 'prefix': None,\n",
      " 'problem_type': None,\n",
      " 'pruned_heads': {},\n",
      " 'remove_invalid_values': False,\n",
      " 'repetition_penalty': 1.0,\n",
      " 'return_dict': True,\n",
      " 'return_dict_in_generate': False,\n",
      " 'sep_token_id': None,\n",
      " 'task_specific_params': None,\n",
      " 'temperature': 1.0,\n",
      " 'tie_encoder_decoder': False,\n",
      " 'tie_word_embeddings': True,\n",
      " 'tokenizer_class': None,\n",
      " 'top_k': 50,\n",
      " 'top_p': 1.0,\n",
      " 'torch_dtype': None,\n",
      " 'torchscript': False,\n",
      " 'transformers_version': '4.11.3',\n",
      " 'type_vocab_size': 2,\n",
      " 'use_bfloat16': False,\n",
      " 'use_cache': True,\n",
      " 'vocab_size': 32000}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")\n",
    "config = model.config\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(config.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f27f93",
   "metadata": {},
   "source": [
    "# âœ…Out of Memory \n",
    "\n",
    "---\n",
    "\n",
    "1. ë¹„ìš´ìƒíƒœ\n",
    "\n",
    "`\n",
    "Total memory     : 14.56 GB\n",
    "Allocated memory : 0.83 GB\n",
    "Reserved memory  : 1.27 GB\n",
    "Usage ratio      : 5.7 %\n",
    "`\n",
    "\n",
    "2. ë°ì´í„° ë¡œë“œ í›„\n",
    "\n",
    "\n",
    "`\n",
    "Total memory     : 14.56 GB\n",
    "Allocated memory : 0.83 GB\n",
    "Reserved memory  : 1.27 GB\n",
    "Usage ratio      : 5.7 %\n",
    "`\n",
    "\n",
    "3. ë¦¬ìŠ¤íƒ€íŠ¸í•´ì„œ ë§ˆì§€ë§‰ì— í•„ìš”í•œ ê²ƒë“¤ë§Œ ì™ì™ ë¹¼ë¨¹ìŒ.\n",
    "\n",
    "```python\n",
    "# Hugging Faceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ğŸ”¹ 1. Tokenizer ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('./nsmc-klue-bert-finetuned')\n",
    "\n",
    "# ğŸ”¹ 2. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ë¶„ë¥˜ìš©)\n",
    "\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    './nsmc-klue-bert-finetuned',  # ì‚¬ì „ í•™ìŠµëœ klue/bert-base ëª¨ë¸ ì‚¬ìš©\n",
    "    num_labels=2                # ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜: 2ê°œ (0 ë˜ëŠ” 1)\n",
    ")\n",
    "```\n",
    "\n",
    "ì—¬ê¸°ì„œ klue-bert-finetuned ë§Œ ê°€ì ¸ì˜´.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1676e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "\n",
    "# # ì‚­ì œí•˜ë ¤ëŠ” ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "# vars_to_delete = [\n",
    "#     'huggingface_model', 'model',\n",
    "#     'trainer', 'trainer_bucketing',\n",
    "#     'hf_train_dataset', 'hf_val_dataset', 'hf_test_dataset'\n",
    "# ]\n",
    "\n",
    "# # ì¡´ì¬í•˜ëŠ” ë³€ìˆ˜ë§Œ ì•ˆì „í•˜ê²Œ ì œê±°\n",
    "# for var_name in vars_to_delete:\n",
    "#     try:\n",
    "#         del globals()[var_name]\n",
    "#     except KeyError:\n",
    "#         pass  # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë³€ìˆ˜ë©´ ë¬´ì‹œ\n",
    "\n",
    "# # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰\n",
    "# gc.collect()\n",
    "\n",
    "# # PyTorch ìºì‹œ ë©”ëª¨ë¦¬ ë¹„ìš°ê¸°\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d85b4ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory     : 14.56 GB\n",
      "Allocated memory : 1.66 GB\n",
      "Reserved memory  : 5.57 GB\n",
      "Usage ratio      : 11.4 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# í˜„ì¬ GPUì˜ ID (ë³´í†µ 0ë²ˆ)\n",
    "gpu_id = 0\n",
    "\n",
    "# ì´ ë©”ëª¨ë¦¬\n",
    "total = torch.cuda.get_device_properties(gpu_id).total_memory\n",
    "\n",
    "# í˜„ì¬ í• ë‹¹ëœ ë©”ëª¨ë¦¬\n",
    "allocated = torch.cuda.memory_allocated(gpu_id)\n",
    "\n",
    "# í˜„ì¬ ìºì‹œëœ ë©”ëª¨ë¦¬ (reserved)\n",
    "reserved = torch.cuda.memory_reserved(gpu_id)\n",
    "\n",
    "print(f\"Total memory     : {total / 1024**3:.2f} GB\")\n",
    "print(f\"Allocated memory : {allocated / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved memory  : {reserved / 1024**3:.2f} GB\")\n",
    "print(f\"Usage ratio      : {allocated / total * 100:.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0e4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
